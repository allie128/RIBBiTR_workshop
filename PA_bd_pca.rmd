---
title: "From variants to PCA with Bd Fluidigm data"
output: github_document
editor_options: 
  chunk_output_type: console
---
Before running this script I first did a few steps call SNPS from our data. I aligned the raw sequencing reads to a set of reference alleles using the program bwa-mem, then called variants using the haplotype-based caller Freebayes. Variants were then filtered using to only include variants with a minor allele frequency > 0.01, quality > 30, less than 10% missing data, and minimum depth of 5. There are plenty of tutorials out there to get from raw reads to filtered variants. But for the sake of time let's just start with out filtered variants in the form of a .vcf file.

As always, let's start with installing the pacakges we will need.

```{r setup, include=FALSE}
install.packages("knitr")
install.packages("devtools")
install.packages("tidyverse")
install.packages("vcfR")
install.packages("ggplot2")
install.packages("adegenet")
install.packages("poppr")
install.packages("RColorBrewer")
install.packages("vegan")
install.packages("Imap")

```

Now let's load the packages.

```{r}

library(knitr)
library(devtools)
library(tidyverse)
library(vcfR)
library(ggplot2)
library(adegenet)
library(poppr)
library(RColorBrewer)
library(vegan)
library(Imap)

```


First we read in our variant call file (VCF) and match it to the metadata table.

```{r}
#read in vcf calcualated using freebayes
Bd.VCF <- read.vcfR("PA_Bd_freebayes_trimmed.vcf")
#read in file with sample metadata
Bd.meta <- read_csv(file = "PA_samples_metadata.csv")

#join in a meta table based on sample ID
colnames(Bd.VCF@gt)[-1] -> vcf.names
as.data.frame(vcf.names) -> vcf.names
colnames(vcf.names) <- "Sample_ID"
left_join(vcf.names, Bd.meta, by = "Sample_ID") -> vcf.meta
#check
all(colnames(Bd.VCF@gt)[-1] == vcf.meta$Sample)
```

Now we are going to turn the VCF into a genlight object for downstream calcualtions.

```{r}
gl.Bd <- vcfR2genlight(Bd.VCF)
ploidy(gl.Bd) <- 2
pop(gl.Bd) <- vcf.meta$GROUP
#get summary of data
gl.Bd
```

Now we are going to use a discriminant analysis of principal components (DAPC) to identify custers within our genetic data using the package adegenet.

```{r}

#this finds the optimal number of clusters for us
grp <- find.clusters(gl.Bd, max.n.clust=10, n.pca = 100, choose.n.clust = F, criterion = "diffNgroup")
 
grp

#if we want to see the BIC chart we can see why it chose 3 because it is the "elbow" of the chart.
#grp <- find.clusters(gl.Bd, max.n.clust=10, n.pca = 100, choose.n.clust = T)
#3

```

Let's run the DAPC with lots of PCs and see what we get.

```{r}
#run with 50 PCs 
dapc1 <- dapc(gl.Bd, grp$grp, n.pca=50, n.da=2)

summary(dapc1)

scatter(dapc1)

```

Wow those groups are really different! BUT this is likely due to overfitting because we are using so many PCs for the calculation. So lets find out how many PCs we should use

```{r}
#then use this to find the optimal number of PCs to use
temp <- optim.a.score(dapc1)
temp <- optim.a.score(dapc1)
temp <- optim.a.score(dapc1)

#we ran it three times and got the same thing. It says our optimal number of PCs is 1 so let's use 2 to retain more information. 

#run the DAPC again now we will just use 2 PCs
dapc1 <- dapc(gl.Bd, grp$grp, n.pca=2, n.da=2)
#to explore split
scatter(dapc1)

```

Now we get a pretty reasonable spread. Perhaps some of these samples don't have a high probability of being group into either 1 or 2. Let's check out the probabilities...

```{r}

#to see assignment probabilities
assignplot(dapc1)

#we can see that samples with yellow boxes have lower pobability of being assigned to either group.

#Let us consider as admixed individuals having no more than 99% of probability of membership in a single cluster:
 
unassign <- which(apply(dapc1$posterior,1, function(e) all(e<0.99)))
unassign <- as.numeric(unassign)

#now make a dataframe with the assignments
assign1 <- as_tibble(as.numeric(dapc1$assign))
assign1[unassign,] <- 0

#now let's add this assignment information to our meta file so we can use it in our PCA
vcf.meta <- cbind(vcf.meta, assign=assign1$value)
```

Now we use the assignments to make a PCA

```{r}
#calculate PCA
pca <- glPca(gl.Bd, nf = 3)
#plot how much variance is explained by the subsequent PCs
barplot(100*pca$eig/sum(pca$eig), col = heat.colors(50), main="PCA Eigenvalues")
title(ylab="Percent of variance\nexplained", line = 2)
title(xlab="Eigenvalues", line = 1)

```

Over 50% of the variation in the data is captured in PC1! Now let's see what the PCA looks like.

```{r}
pca.scores <- as.data.frame(pca$scores)
cols <- brewer.pal(n_distinct(vcf.meta$assign), "Set1")

p <- ggplot(pca.scores, aes(x=PC2, y=PC1, colour=as.factor(vcf.meta$assign), shape=vcf.meta$Species_code)) + 
  geom_point(size=5) + 
  scale_color_manual(values = c("dark grey",cols[2],cols[1],cols[3])) + 
  scale_shape_manual(values = c(0,2,10,16,3,4,6,7,8,9,15,11,12)) +
  stat_ellipse(aes(x=PC2, y=PC1, group=as.factor(assign1$value)),level = 0.95, size = 1) + 
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0) + 
  theme_bw()

p

```

Now we can compare the results we just calcualted with those calculates by me based on the larger dataset of Nevada and PA sample for our recent [plos one paper](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0261047).

```{r}

p2 <- ggplot(pca.scores, aes(x=PC2, y=PC1, colour=as.factor(vcf.meta$GENOASSIGN), shape=vcf.meta$Species_code)) + 
  geom_point(size=5) + 
  scale_color_manual(values = c(cols[2],cols[1],"dark grey")) + 
  scale_shape_manual(values = c(0,2,10,16,3,4,6,7,8,9,15,11,12)) +
  stat_ellipse(aes(x=PC2, y=PC1, group=as.factor(assign1$value)),level = 0.95, size = 1) + 
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0) + 
  theme_bw()

p2

```

We can see that the unassigned are the middle group while the GPL1 and GPL2 genotypes are on each side.

**Run AMOVA to test variation of genetic data based on site or species**

Sourced from this [tutorial](https://grunwaldlab.github.io/poppr/reference/poppr.amova.html)

```{r}
#use the package poppr
pa_genind <- vcfR2genind(Bd.VCF)

#set strata as site and species
pa_strata <- data.frame(cbind(vcf.meta$Site_code,vcf.meta$Species_code, 
paste(vcf.meta$Site_code,vcf.meta$Species_code, sep="_")))
colnames(pa_strata) <- c("Site","Species","Site_Species")
strata(pa_genind) <- pa_strata
pa_genclone <- as.genclone(pa_genind)

#check out sample numbers
table(strata(pa_genclone, ~Site/Species, combine = FALSE))

#run amova
PA_amova <- poppr.amova(pa_genclone, ~Site/Species)

#check it out
PA_amova

#run significance test
set.seed(1989)
PA_signif   <- randtest(PA_amova, nrepet = 999)
plot(PA_signif)

PA_signif

```

From this we can see that a high proportion of variance (64%) is explained by variations between samples (i.e. species) within Site. and the p-value is signifcant.

**Calculate heterozygosity among different assigned genotypes**

First, I use the program vcftools to run the following command on my input vcf:

> ../vcftools --vcf PA_Bd_freebayes_trimmed.vcf --het --out PA_Bd

This gave me the output called "PA_Bd.het" which I will read in here.

```{r}
#read in
het_all <- read_delim("PA_Bd.het", delim = "\t",
           col_names = c("Sample_ID","ho", "he", "nsites", "f"), skip = 1)

#join to other metadata
left_join(het_all, vcf.meta, by = "Sample_ID") -> vcf.meta.het
vcf.meta.het <- mutate(vcf.meta.het, ho_calc=1-(ho/nsites))

#plot by genotype
p <- ggplot(vcf.meta.het, aes(x=as.factor(GENOASSIGN), y=ho_calc, color=as.factor(GENOASSIGN))) + 
  geom_boxplot()+
  xlab("Genotype")+
  ylab("Individual Heterozygosity")+
  scale_color_manual(values = c(cols[2],cols[1],"dark grey")) +
  theme_bw()

p + geom_dotplot(binaxis='y', stackdir='center', dotsize=0.5)


```

**Calculate pairwise genetic distance and plot vs geographic distance**

```{r}
#calculate pairwise genetic distance
pa.dist <- poppr::bitwise.dist(gl.Bd, mat=T)

#calcualte geo dist
pa_pts <- cbind(vcf.meta$Sample_ID,vcf.meta$Lat,vcf.meta$Lon)
colnames(pa_pts) <- c("name","lat","lon")
write.csv(pa_pts, file="pa_geo_pts.csv")
samples_loc_pa <- read.csv("pa_geo_pts.csv", header = T)
#functions for calculating geo dist
ReplaceLowerOrUpperTriangle <- function(m, triangle.to.replace){
   # If triangle.to.replace="lower", replaces the lower triangle of a square matrix with its upper triangle.
   # If triangle.to.replace="upper", replaces the upper triangle of a square matrix with its lower triangle.
   if (nrow(m) != ncol(m)) stop("Supplied matrix must be square.")
   if      (tolower(triangle.to.replace) == "lower") tri <- lower.tri(m)
   else if (tolower(triangle.to.replace) == "upper") tri <- upper.tri(m)
   else stop("triangle.to.replace must be set to 'lower' or 'upper'.")
   m[tri] <- t(m)[tri]
   return(m)
}
GeoDistanceInMetresMatrix <- function(df.geopoints){
   # Returns a matrix (M) of distances between geographic points.
   # M[i,j] = M[j,i] = Distance between (df.geopoints$lat[i], df.geopoints$lon[i]) and
   # (df.geopoints$lat[j], df.geopoints$lon[j]).
   # The row and column names are given by df.geopoints$name.
   GeoDistanceInMetres <- function(g1, g2){
      # Returns a vector of distances. (But if g1$index > g2$index, returns zero.)
      # The 1st value in the returned vector is the distance between g1[[1]] and g2[[1]].
      # The 2nd value in the returned vector is the distance between g1[[2]] and g2[[2]]. Etc.
      # Each g1[[x]] or g2[[x]] must be a list with named elements "index", "lat" and "lon".
      # E.g. g1 <- list(list("index"=1, "lat"=12.1, "lon"=10.1), list("index"=3, "lat"=12.1, "lon"=13.2))
      DistM <- function(g1, g2){
         require("Imap")
         return(ifelse(g1$index > g2$index, 0, gdist(lat.1=g1$lat, lon.1=g1$lon, lat.2=g2$lat, lon.2=g2$lon, units="m")))
      }
      return(mapply(DistM, g1, g2))
   }
   n.geopoints <- nrow(df.geopoints)
   # The index column is used to ensure we only do calculations for the upper triangle of points
   df.geopoints$index <- 1:n.geopoints
   # Create a list of lists
   list.geopoints <- by(df.geopoints[,c("index", "lat", "lon")], 1:n.geopoints, function(x){return(list(x))})
   # Get a matrix of distances (in metres)
   mat.distances <- ReplaceLowerOrUpperTriangle(outer(list.geopoints, list.geopoints, GeoDistanceInMetres), "lower")
   # Set the row and column names
   rownames(mat.distances) <- df.geopoints$name
   colnames(mat.distances) <- df.geopoints$name
   return(mat.distances)
}

#calculate the distance matrix

distance.mat.m.pa <- GeoDistanceInMetresMatrix(samples_loc_pa)

#check the dimensions (these should match)
dim(pa.dist)
dim(distance.mat.m.pa)

geo_dist_pa <- distance.mat.m.pa[lower.tri(distance.mat.m.pa)]
gen_dist_pa <- pa.dist[lower.tri(pa.dist)]

#for plotting a linear model on the data
gen_dist_dist_pa <- as.dist(pa.dist)
geo_km_dist_dist_pa <- as.dist(distance.mat.m.pa)
pa_lm <- lm(gen_dist_dist_pa ~ geo_km_dist_dist_pa)
#intercept
intercept <- pa_lm$coefficients[1]
#slope for km
slope <- pa_lm$coefficients[2]*1000

#now we can plot
plot(geo_dist_pa/1000, gen_dist_pa, xlab="Geographic Distance (km)", ylab="Genetic Distance")
abline(intercept, slope, col = "gray", lty = 3, lwd=2)

```

Finally we can run a mantel test to see if geo and genetic distances are correlated in this system.

```{r}
#mantel test
mantel(distance.mat.m.pa, pa.dist)

```

Now we can see that there is a very weak correaltion betwen Bd genetic distance and geographic distance (r = 0.1134). 


